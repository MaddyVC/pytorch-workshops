{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport csv\nimport glob\nimport PIL\nclass Object(object): \n    pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport csv\nimport glob\nimport PIL\nclass Object(object): \n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reorganize image folders\n```\n /train/\n    label1/\n       train_1.jpg\n       train_2.jpg\n    label2/\n       ...\n /test\n    /label1/\n       ...\n    /label2/\n```"},{"metadata":{},"cell_type":"markdown","source":"IMAGE_PATH = \"../input/plant-pathology-2020-fgvc7\"\nOUTPUT_PATH = \"../working/plant-pathology-2020-fgvc7\"\ninput_path = OUTPUT_PATH"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, csv, shutil\n\ndef move_files(root_path):\n    image_class = {}\n    classes = []\n    class_pos = {}\n    class_image = {}\n    \n    csv_name = 'train.csv'\n    with open(f\"{root_path}/{csv_name}\", \"rt\") as f:\n        first = True\n        for line in csv.reader(f):\n            if first:\n                for i, c in enumerate(line):\n                    if i > 0:\n                        class_pos.setdefault(i - 1, c)\n                        classes.append(c)\n            else:\n                for i, c in enumerate(line):\n                    if i > 0 and int(c) == 1:\n                        file_name = line[0]\n                        image_class.setdefault(file_name, i - 1)\n                        img_class_name = class_pos[i - 1]\n                        class_image.setdefault(img_class_name, [])\n                        class_image[img_class_name].append(file_name)\n            first = False\n\n    out_path = OUTPUT_PATH\n    types = [\"train\", \"test\"]\n    for t in types:\n        dir_path = f\"{out_path}/{t}\"\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        for c in classes:\n            sub_path = dir_path + \"/\" + c\n            if not os.path.exists(sub_path):\n                os.makedirs(sub_path)\n\n    # files into test: 70 or 20%, whicheve is smaller\n    for class_name in class_image:\n        images = class_image[class_name]\n        test_size = int(min(70, 0.2 * len(images)))\n        sub_path = f\"{out_path}/train/{class_name}\"\n        for fname in images[test_size : ]:\n            src = f\"{root_path}/images/{fname}.jpg\"\n            dst = f\"{sub_path}/{fname}.jpg\"\n            shutil.copyfile(src, dst)\n        sub_path = f\"{out_path}/test/{class_name}\"\n        for fname in images[: test_size]:\n            src = f\"{root_path}/images/{fname}.jpg\"\n            dst = f\"{sub_path}/{fname}.jpg\"\n            shutil.copyfile(src, dst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"move_files(IMAGE_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig = Object()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model = torchvision.models.resnet50(pretrained=True).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replace the classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model.fc = nn.Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = orig.model.requires_grad_(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = Object()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.transforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, .224, .225])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.train = torchvision.datasets.ImageFolder(\n    input_path + '/train', \n    transform=tr.transforms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.valid = torchvision.datasets.ImageFolder(\n    input_path + '/test', \n    transform=tr.transforms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tr.train.samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.train_loader = torch.utils.data.DataLoader(\n    tr.train, \n    batch_size=32,\n    shuffle=True,\n    num_workers=2,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.valid_loader = torch.utils.data.DataLoader(\n    tr.valid, \n    batch_size=32,\n    shuffle=True,\n    num_workers=2,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.train_loader.dataset.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embeddings(loader, model):\n    train_embeddings = None\n    train_y = None\n\n    model.eval()\n\n    batch_num = 1\n    \n    # The loop will read the set of images and corresponding\n    # labels one batch at a time, or 32 images/labels at a time.\n    # (see the batch_size value above for the actual size).\n    for x, y in loader:\n        if batch_num % 5 == 1:\n            print('Batch:', batch_num)\n        batch_num += 1;\n        \n        # Calculate outputs for all 16 images in a batch.\n        # Rembember that outputs are the values produce by the\n        # layer one before the last. Typically these values \n        # are called \"embeddings\".\n        # It will be an array of 2048 real values per image.        \n        batch_embeddings = model(x.to(device))\n\n        # Concatenate new tensors to get one continuous array\n        if train_embeddings is None:\n            train_embeddings = batch_embeddings\n        else:\n            train_embeddings = torch.cat(\n                (train_embeddings, batch_embeddings),\n                0)\n\n        if train_y is None:\n            train_y = y.to(device)\n        else:\n            train_y = torch.cat(\n                (train_y, y.to(device)),\n                0)\n\n    print(train_y.shape,\n          train_embeddings.shape)\n\n    return train_y, train_embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntr.train_y, tr.train_embeddings = \\\n    build_embeddings(tr.train_loader, orig.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntr.test_y, tr.test_embeddings = \\\n    build_embeddings(tr.valid_loader, orig.model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.train_embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_model( model,\n                    criterion,\n                    x,\n                    y):\n    model.eval()\n    outputs = model(x)\n    loss = criterion(outputs, y)\n    _, preds = torch.max(outputs, 1)\n    epoch_loss = loss.item()\n    epoch_acc = torch.sum(preds == y.data).double() / x.size(0)\n    print(f'Validation loss: {epoch_loss:.4f}, accuracy: {epoch_acc:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.criterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_embeddings(\n    model,\n    epochs,\n    batch_size, \n    train_emb,\n    valid_emb,\n    train_y,\n    valid_y):\n    \n    criterion = tr.criterion\n    opt = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, nesterov=True)\n    \n    report_every = min(3000, epochs / 10)\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        running_correct = 0\n        \n        steps = int(train_emb.size(0) / batch_size + 1)\n        \n        for bi in range(steps):\n            start = bi * batch_size\n            end = (bi + 1) * batch_size\n            \n            x = train_emb[start : end]\n            y = train_y[start : end]\n            \n            outputs = model(x)\n            \n            loss = criterion(outputs, y)\n            \n            opt.zero_grad()\n            \n            loss.backward()\n            \n            opt.step()\n            \n            _, preds = torch.max(outputs, 1)\n            \n            running_loss += loss.item() * x.size(0)\n            running_correct += torch.sum(preds == y)\n            \n        if epoch % report_every == 1:\n            epoch_loss = running_loss / train_emb.size(0)\n            epoch_acc = running_correct.double() / train_emb.size(0)\n            print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.4f}',\n                  f', accuracy: {epoch_acc:.4f}')\n            \n    epoch_loss = running_loss / train_emb.size(0)\n    epoch_acc = running_correct.double() / train_emb.size(0)\n    print(f'Batch Size: {batch_size}, epochs: {epochs}')\n    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.fc = nn.Linear(2048, 4).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.fc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_model_embeddings(\n    tr.fc,\n    1000,\n    32,\n    tr.train_embeddings,\n    tr.test_embeddings,\n    tr.train_y,\n    tr.test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_model(tr.fc, tr.criterion, tr.test_embeddings, tr.test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.test_images = [\n    OUTPUT_PATH + '/test/healthy/Train_100.jpg',\n    OUTPUT_PATH + '/test/multiple_diseases/Train_122.jpg',\n    OUTPUT_PATH + '/test/rust/Train_10.jpg',\n    OUTPUT_PATH + '/test/rust/Train_102.jpg',\n    OUTPUT_PATH + '/test/scab/Train_11.jpg',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\ntr.img_list = [Image.open(img_path).convert(\"RGB\") for img_path in tr.test_images]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.test_batch = torch.stack([\n    tr.transforms(img).to(device) for img in tr.img_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.test_batch.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model.eval()\ntr.logits = orig.model(tr.test_batch)\ntr.logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.logits.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model.fc = tr.fc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model.eval()\ntr.logits = orig.model(tr.test_batch)\ntr.logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import functional as F\nF.softmax(tr.logits, dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.train_loader.dataset.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.proba = F.softmax(tr.logits, dim=1).cpu().data.numpy()\ntr.proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ntr.fig, tr.axs = plt.subplots(1, len(tr.img_list), figsize=(20, 5))\nfor i, img in enumerate(tr.img_list):\n    ax = tr.axs[i]\n    ax.axis('off')\n    \n    ax.set_title(\"{:.0f}% {}, {:.0f}% {}\\n {:.0f}% {}, {:.0f}% {}\"\n                 .format(100 * tr.proba[i,0], tr.train_loader.dataset.classes[0],\n                         100 * tr.proba[i,1], tr.train_loader.dataset.classes[1],\n                         100 * tr.proba[i,2], tr.train_loader.dataset.classes[2],\n                         100 * tr.proba[i,3], tr.train_loader.dataset.classes[3],\n                  )\n    )\n    \n    ax.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.test_images = [\n    OUTPUT_PATH + '/test/healthy/Train_100.jpg',\n    OUTPUT_PATH + '/test/multiple_diseases/Train_122.jpg',\n    OUTPUT_PATH + '/test/rust/Train_10.jpg',\n    OUTPUT_PATH + '/test/rust/Train_102.jpg',\n    OUTPUT_PATH + '/test/scab/Train_11.jpg',\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert To Production"},{"metadata":{"trusted":true},"cell_type":"code","source":"orig.model_script = torch.jit.trace(orig.model, tr.test_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = orig.model_script.cpu()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ml2rt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ml2rt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ml2rt.save_torch(orig.model_script, 'plant.pt')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}