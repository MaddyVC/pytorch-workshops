{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/huan9huan/walk-or-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "class Object(object): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model = models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.fc = nn.Sequential()\n",
    "_ = orig.model.requires_grad_(False)\n",
    "_ = orig.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Images\n",
    "\n",
    "Reorgnize images on the disk so that they can be quickly downloaded by ImageFolder/ImageLoader.\n",
    "\n",
    "```\n",
    " /train/\n",
    "    label1/\n",
    "       train_1.jpg\n",
    "       train_2.jpg\n",
    "    label2/\n",
    "       ...\n",
    " /test\n",
    "    /label1/\n",
    "       ...\n",
    "    /label2/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, shutil\n",
    "\n",
    "IMAGE_PATH=\"../input/plant-pathology-2020-fgvc7\"\n",
    "\n",
    "def move_files(root_path):\n",
    "    image_class = {}\n",
    "    classes = []\n",
    "    class_pos = {}\n",
    "    class_image = {}\n",
    "    \n",
    "    csv_name = 'train.csv'\n",
    "    with open(f\"{root_path}/{csv_name}\", \"rt\") as f:\n",
    "        first = True\n",
    "        for line in csv.reader(f):\n",
    "            if first:\n",
    "                for i, c in enumerate(line):\n",
    "                    if i > 0:\n",
    "                        class_pos.setdefault(i - 1, c)\n",
    "                        classes.append(c)\n",
    "            else:\n",
    "                for i, c in enumerate(line):\n",
    "                    if i > 0 and int(c) == 1:\n",
    "                        file_name = line[0]\n",
    "                        image_class.setdefault(file_name, i - 1)\n",
    "                        img_class_name = class_pos[i - 1]\n",
    "                        class_image.setdefault(img_class_name, [])\n",
    "                        class_image[img_class_name].append(file_name)\n",
    "            first = False\n",
    "                        \n",
    "    types = [\"train\", \"test\"]\n",
    "    for t in types:\n",
    "        dir_path = f\"{root_path}/{t}\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        for c in classes:\n",
    "            sub_path = dir_path + \"/\" + c\n",
    "            if not os.path.exists(sub_path):\n",
    "                os.makedirs(sub_path)\n",
    "\n",
    "    # files into test: 70 or 20%, whicheve is smaller\n",
    "    for class_name in class_image:\n",
    "        images = class_image[class_name]\n",
    "        test_size = int(min(70, 0.2 * len(images)))\n",
    "        sub_path = f\"{root_path}/train/{class_name}\"\n",
    "        for fname in images[test_size : ]:\n",
    "            src = f\"{root_path}/images/{fname}.jpg\"\n",
    "            dst = f\"{sub_path}/{fname}.jpg\"\n",
    "            shutil.copyfile(src, dst)\n",
    "        sub_path = f\"{root_path}/test/{class_name}\"\n",
    "        for fname in images[: test_size]:\n",
    "            src = f\"{root_path}/images/{fname}.jpg\"\n",
    "            dst = f\"{sub_path}/{fname}.jpg\"\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../input/walk-or-run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, .224, .225])\n",
    "tr.transforms_train = transforms.Compose([\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    tr.normalize\n",
    "])\n",
    "tr.transforms_valid = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    tr.normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train = datasets.ImageFolder(input_path + '/walk_or_run_train/train', transform=tr.transforms_train)\n",
    "tr.valid = datasets.ImageFolder(input_path + '/walk_or_run_test/test', transform=tr.transforms_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr.train.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader = torch.utils.data.DataLoader(tr.train, batch_size=32, num_workers=4)\n",
    "tr.valid_loader = torch.utils.data.DataLoader(tr.valid, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load images in batches, calculate, and store intermediate outputs for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr.train_embeddings = []\n",
    "tr.y = []\n",
    "\n",
    "# The loop will read the set of images and corresponding\n",
    "# labels one batch at a time, or 16 images/labels at a time.\n",
    "# (see the batch_size value above for the actual size).\n",
    "for x, y in tr.train_loader:\n",
    "    # x.shape == [16, 3, 224, 224]\n",
    "    # y.shape == [16]\n",
    "    \n",
    "    # Calculate outputs for all 16 images in a batch.\n",
    "    # Rembember that outputs are the values produce by the\n",
    "    # layer one before the last. Typically these values \n",
    "    # are called \"embeddings\".\n",
    "    # It will be an array of 2048 real values per image.\n",
    "    tr.batch_embeddings = orig.model(x.to(device))\n",
    "    print(tr.batch_embeddings.shape, y)\n",
    "    \n",
    "    # Eventually we want to create a continious tensor of shape\n",
    "    # [600, 2038] for all embeddings. To do this we need to break\n",
    "    # down batches into the list of individual tensors, e.g.\n",
    "    # [16, 2048] -> [[2048], [2048], ...]\n",
    "    \n",
    "    tr.batch_embeddings = tr.batch_embeddings.unbind()\n",
    "    \n",
    "    # Accumulate embeddings in a list\n",
    "    tr.train_embeddings.extend(tr.batch_embeddings)\n",
    "    tr.y.extend(y.unbind())\n",
    "    \n",
    "    print(len(tr.train_embeddings), len(tr.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_embeddings = torch.stack(tr.train_embeddings)\n",
    "tr.train_y = torch.stack(tr.y)\n",
    "tr.train_embeddings.shape, tr.train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr.valid_embeddings = []\n",
    "tr.valid_y = []\n",
    "tr.X_valid = []\n",
    "\n",
    "for x, y in tr.valid_loader:\n",
    "    tr.batch_embeddings = orig.model(x.to(device)).unbind()\n",
    "    tr.valid_embeddings.extend(tr.batch_embeddings)\n",
    "    tr.valid_y.extend(y.unbind())\n",
    "    tr.X_valid.extend(x.unbind())\n",
    "    \n",
    "tr.batch_embeddings = None\n",
    "tr.valid_embeddings = torch.stack(tr.valid_embeddings)\n",
    "tr.valid_y = torch.stack(tr.valid_y)\n",
    "tr.X_valid = torch.stack(tr.X_valid)\n",
    "\n",
    "print(tr.X_valid.shape, tr.valid_embeddings.shape, tr.valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding for each image is an array of 2048 floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.valid_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(model, criterion, opt, epochs, train_emb, valid_emd, train_y, valid_y):\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1} of {epochs}')\n",
    "        print('******************************')\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_correct = 0\n",
    "        \n",
    "        x = train_emb\n",
    "        y = train_y\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        running_correct += torch.sum(preds==y.data)\n",
    "        epoch_loss = running_loss / train_emb.shape[0]\n",
    "        epoch_acc = running_correct.double() / train_emb.shape[0]\n",
    "        print('Training loss: {:.4f}, accuracy: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0\n",
    "        running_correct = 0\n",
    "        x = valid_emd\n",
    "        y = valid_y\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        running_correct += torch.sum(preds==y.data)\n",
    "        epoch_loss = running_loss / valid_emd.shape[0]\n",
    "        epoch_acc = running_correct.double() / valid_emd.shape[0]\n",
    "        print('Validation loss: {:.4f}, accuracy: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "tr.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2)\n",
    ").to(device)\n",
    "\n",
    "tr.optimizer = optim.SGD(tr.fc.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "_ = train_embeddings(\n",
    "    tr.fc, \n",
    "    tr.criterion, \n",
    "    tr.optimizer, \n",
    "    90, \n",
    "    tr.train_embeddings, \n",
    "    tr.valid_embeddings,\n",
    "    tr.train_y, \n",
    "    tr.valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.fc = tr.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr.test_images = [\n",
    "#     input_path + '/walk_or_run_test/test/run/run_78b39d88.png',\n",
    "#     input_path + '/walk_or_run_test/test/run/run_365fa2e5.png',\n",
    "#     input_path + '/walk_or_run_test/test/run/run_603ac08a.png',\n",
    "#     input_path + '/walk_or_run_test/test/walk/walk_9e646bbc.png',\n",
    "#     input_path + '/walk_or_run_test/test/walk/walk_443d602c.png',\n",
    "# ]\n",
    "\n",
    "tr.test_images = [\n",
    "    OUTPUT_PATH + '/test/healthy/Train_100.jpg',\n",
    "    OUTPUT_PATH + '/test/multiple_diseases/Train_122.jpg',\n",
    "    OUTPUT_PATH + '/test/rust/Train_10.jpg',\n",
    "    OUTPUT_PATH + '/test/rust/Train_102.jpg',\n",
    "    OUTPUT_PATH + '/test/scab/Train_11.jpg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "tr.img_list = [Image.open(img_path).convert(\"RGB\") for img_path in tr.test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.test_batch = torch.stack([\n",
    "    tr.transforms_valid(img).to(device) for img in tr.img_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.logits = orig.model(tr.test_batch)\n",
    "tr.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "tr.proba = F.softmax(tr.logits, dim=1).cpu().data.numpy()\n",
    "tr.proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.fig, tr.axs = plt.subplots(1, len(tr.img_list), figsize=(20, 5))\n",
    "for i, img in enumerate(tr.img_list):\n",
    "    ax = tr.axs[i]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.set_title(\"{:.0f}% {}, {:.0f}% {}\\n {:.0f}% {}, {:.0f}% {}\"\n",
    "                 .format(100 * tr.proba[i,0], tr.train_loader.dataset.classes[0],\n",
    "                         100 * tr.proba[i,1], tr.train_loader.dataset.classes[1],\n",
    "                         100 * tr.proba[i,2], tr.train_loader.dataset.classes[2],\n",
    "                         100 * tr.proba[i,3], tr.train_loader.dataset.classes[3],\n",
    "                  )\n",
    "    )\n",
    "    \n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model for Redis AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml2rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will evaluate and convert model into a special TorchScript format. This format is universal and is longer tied to Python. So, we can load it inside Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.eval()\n",
    "tr.nn_script = torch.jit.trace(orig.model, tr.X_valid[0:10].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we can use TorchScript model just like any other PyTorch model. Now we can save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.nn_script.eval()\n",
    "ml2rt.save_torch(tr.nn_script, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model into Redis AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redisai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.redis = redisai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tr.redis.loadbackend('TORCH', 'redisai_torch/redisai_torch.so')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.loaded_model = ml2rt.load_model('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.redis.modelset(\n",
    "    \"walk_or_run\", \n",
    "    redisai.Backend.torch,\n",
    "    redisai.Device.cpu,\n",
    "    tr.loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input/walk-or-run/walk_or_run_test/test/walk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.img = Image \\\n",
    "    .open(input_path + '/walk_or_run_test/test/walk/walk_9d193f21.png') \\\n",
    "    .convert(\"RGB\")\n",
    "tr.img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model expects images in a certain format:\n",
    "   * 224 x 224\n",
    "   * Colors re-scaled the mean and std dev:\n",
    "     * mean=[0.485, 0.456, 0.406]\n",
    "     * std=[0.229, .224, .225]\n",
    "   * The image must also be a part of an array, even if it's just one image. We will insert another dimension as the beginning to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.img_rescaled = tr.transforms_valid(tr.img)\n",
    "tr.img_rescaled = np.expand_dims(tr.img_rescaled, axis=0)\n",
    "tr.img_rescaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the image to Redis AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.redis.tensorset('input', tr.img_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.redis.modelrun('walk_or_run', 'input', 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.redis.tensorget('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.Tensor(tr.redis.tensorget('pred')), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classes = ['run', 'walk'], so the prediction is **'walk'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3b6c6579d5a64ba896c2682ad66590d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "536c08fedb5744ffa481b853ffff692b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7996b8ba9c5843ceb1937c062bbcbdba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c46f8538652745bda7753abc0b882748",
        "IPY_MODEL_efa554b7d5284a13b1545d94d368fe96"
       ],
       "layout": "IPY_MODEL_a13247311847441db00e275f8d1ea137"
      }
     },
     "a13247311847441db00e275f8d1ea137": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c46f8538652745bda7753abc0b882748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_536c08fedb5744ffa481b853ffff692b",
       "max": 241530880,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e3d15ed4f3264d0bbd57f29866dd3b69",
       "value": 241530880
      }
     },
     "e3d15ed4f3264d0bbd57f29866dd3b69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "efa554b7d5284a13b1545d94d368fe96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3b6c6579d5a64ba896c2682ad66590d4",
       "placeholder": "​",
       "style": "IPY_MODEL_feedf67259204cbb81bb4c008221e078",
       "value": " 230M/230M [00:15&lt;00:00, 15.5MB/s]"
      }
     },
     "feedf67259204cbb81bb4c008221e078": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
