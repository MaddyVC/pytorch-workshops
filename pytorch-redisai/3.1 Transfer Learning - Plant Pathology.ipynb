{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import glob\n",
    "import PIL\n",
    "class Object(object): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize image folders\n",
    "```\n",
    " /train/\n",
    "    label1/\n",
    "       train_1.jpg\n",
    "       train_2.jpg\n",
    "    label2/\n",
    "       ...\n",
    " /test\n",
    "    /label1/\n",
    "       ...\n",
    "    /label2/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"../input/plant-pathology-2020-fgvc7\"\n",
    "OUTPUT_PATH = \"../working/plant-pathology-2020-fgvc7\"\n",
    "input_path = OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, shutil\n",
    "\n",
    "def move_files(root_path):\n",
    "    image_class = {}\n",
    "    classes = []\n",
    "    class_pos = {}\n",
    "    class_image = {}\n",
    "    \n",
    "    csv_name = 'train.csv'\n",
    "    with open(f\"{root_path}/{csv_name}\", \"rt\") as f:\n",
    "        first = True\n",
    "        for line in csv.reader(f):\n",
    "            if first:\n",
    "                for i, c in enumerate(line):\n",
    "                    if i > 0:\n",
    "                        class_pos.setdefault(i - 1, c)\n",
    "                        classes.append(c)\n",
    "            else:\n",
    "                for i, c in enumerate(line):\n",
    "                    if i > 0 and int(c) == 1:\n",
    "                        file_name = line[0]\n",
    "                        image_class.setdefault(file_name, i - 1)\n",
    "                        img_class_name = class_pos[i - 1]\n",
    "                        class_image.setdefault(img_class_name, [])\n",
    "                        class_image[img_class_name].append(file_name)\n",
    "            first = False\n",
    "\n",
    "    out_path = OUTPUT_PATH\n",
    "    types = [\"train\", \"test\"]\n",
    "    for t in types:\n",
    "        dir_path = f\"{out_path}/{t}\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        for c in classes:\n",
    "            sub_path = dir_path + \"/\" + c\n",
    "            if not os.path.exists(sub_path):\n",
    "                os.makedirs(sub_path)\n",
    "\n",
    "    # files into test: 70 or 20%, whicheve is smaller\n",
    "    for class_name in class_image:\n",
    "        images = class_image[class_name]\n",
    "        test_size = int(min(70, 0.2 * len(images)))\n",
    "        sub_path = f\"{out_path}/train/{class_name}\"\n",
    "        for fname in images[test_size : ]:\n",
    "            src = f\"{root_path}/images/{fname}.jpg\"\n",
    "            dst = f\"{sub_path}/{fname}.jpg\"\n",
    "            shutil.copyfile(src, dst)\n",
    "        sub_path = f\"{out_path}/test/{class_name}\"\n",
    "        for fname in images[: test_size]:\n",
    "            src = f\"{root_path}/images/{fname}.jpg\"\n",
    "            dst = f\"{sub_path}/{fname}.jpg\"\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model = torchvision.models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.fc = nn.Sequential()\n",
    "_ = orig.model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, .224, .225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train = torchvision.datasets.ImageFolder(\n",
    "    input_path + '/train', \n",
    "    transform=tr.transforms)\n",
    "\n",
    "tr.valid = torchvision.datasets.ImageFolder(\n",
    "    input_path + '/test', \n",
    "    transform=tr.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr.train.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader = torch.utils.data.DataLoader(\n",
    "    tr.train, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,)\n",
    "\n",
    "tr.valid_loader = torch.utils.data.DataLoader(\n",
    "    tr.valid, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(loader, model):\n",
    "    train_embeddings = None\n",
    "    train_y = None\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_num = 1\n",
    "    \n",
    "    # The loop will read the set of images and corresponding\n",
    "    # labels one batch at a time, or 16 images/labels at a time.\n",
    "    # (see the batch_size value above for the actual size).\n",
    "    for x, y in loader:\n",
    "        if batch_num % 5 == 1:\n",
    "            print('Batch:', batch_num)\n",
    "        batch_num += 1;\n",
    "        \n",
    "        # Calculate outputs for all 16 images in a batch.\n",
    "        # Rembember that outputs are the values produce by the\n",
    "        # layer one before the last. Typically these values \n",
    "        # are called \"embeddings\".\n",
    "        # It will be an array of 2048 real values per image.        \n",
    "        batch_embeddings = model(x.to(device))\n",
    "\n",
    "        # Concatenate new tensors to get one continuous array\n",
    "        if train_embeddings is None:\n",
    "            train_embeddings = batch_embeddings\n",
    "        else:\n",
    "            train_embeddings = torch.cat(\n",
    "                (train_embeddings, batch_embeddings),\n",
    "                0)\n",
    "\n",
    "        if train_y is None:\n",
    "            train_y = y.to(device)\n",
    "        else:\n",
    "            train_y = torch.cat(\n",
    "                (train_y, y.to(device)),\n",
    "                0)\n",
    "\n",
    "    print(train_y.shape,\n",
    "          train_embeddings.shape)\n",
    "\n",
    "    return train_y, train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr.train_y, tr.train_embeddings = \\\n",
    "    build_embeddings(tr.train_loader, orig.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr.test_y, tr.test_embeddings = \\\n",
    "    build_embeddings(tr.valid_loader, orig.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to validate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model( model,\n",
    "                    criterion,\n",
    "                    x,\n",
    "                    y):\n",
    "    model.eval()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    epoch_loss = loss.item()\n",
    "    epoch_acc = torch.sum(preds == y.data).double() / x.size(0)\n",
    "    print(f'Validation loss: {epoch_loss:.4f}, accuracy: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_embeddings(\n",
    "    model,\n",
    "    epochs,\n",
    "    batch_size, \n",
    "    train_emb,\n",
    "    valid_emb,\n",
    "    train_y,\n",
    "    valid_y):\n",
    "    \n",
    "    criterion = tr.criterion\n",
    "    opt = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    report_every = min(3000, epochs / 10)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_correct = 0\n",
    "        \n",
    "        steps = int(train_emb.size(0) / batch_size + 1)\n",
    "        \n",
    "        for bi in range(steps):\n",
    "            start = bi * batch_size\n",
    "            end = (bi + 1) * batch_size\n",
    "            \n",
    "            x = train_emb[start : end]\n",
    "            y = train_y[start : end]\n",
    "            \n",
    "            outputs = model(x)\n",
    "            \n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            running_correct += torch.sum(preds == y)\n",
    "            \n",
    "        if epoch % report_every == 1:\n",
    "            epoch_loss = running_loss / train_emb.size(0)\n",
    "            epoch_acc = running_correct.double() / train_emb.size(0)\n",
    "            print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.4f}',\n",
    "                  f', accuracy: {epoch_acc:.4f}')\n",
    "            \n",
    "    epoch_loss = running_loss / train_emb.size(0)\n",
    "    epoch_acc = running_correct.double() / train_emb.size(0)\n",
    "    print(f'Batch Size: {batch_size}, epochs: {epochs}')\n",
    "    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model\n",
    "\n",
    "Let's create a new classification layer. The # of inputs must match the outputs from the frozen part of the model. The # outputs must match the # of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.fc = nn.Linear(2048, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_model_embeddings(\n",
    "    tr.fc,\n",
    "    1000,\n",
    "    32,\n",
    "    tr.train_embeddings,\n",
    "    tr.test_embeddings,\n",
    "    tr.train_y,\n",
    "    tr.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we minimize the training error. That's how the algorithm works. But, it's easy to make the model fit the training data too \"naively\", and have actually poor peformance on new data.\n",
    "\n",
    "To measure this performance we \"hid\" a little bit of data. This is our test set. We can use it to test the likely real-world performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(tr.fc, tr.criterion, tr.test_embeddings, tr.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.test_images = [\n",
    "    OUTPUT_PATH + '/test/healthy/Train_100.jpg',\n",
    "    OUTPUT_PATH + '/test/multiple_diseases/Train_122.jpg',\n",
    "    OUTPUT_PATH + '/test/rust/Train_10.jpg',\n",
    "    OUTPUT_PATH + '/test/rust/Train_102.jpg',\n",
    "    OUTPUT_PATH + '/test/scab/Train_11.jpg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "tr.img_list = [Image.open(img_path).convert(\"RGB\") for img_path in tr.test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.test_batch = torch.stack([\n",
    "    tr.transforms(img).to(device) for img in tr.img_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.eval()\n",
    "tr.logits = orig.model(tr.test_batch)\n",
    "tr.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forgot to attach our new classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model.fc = tr.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "orig.model.eval()\n",
    "tr.logits = orig.model(tr.test_batch)\n",
    "tr.proba = F.softmax(tr.logits, dim=1).cpu().data.numpy()\n",
    "tr.proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tr.fig, tr.axs = plt.subplots(1, len(tr.img_list), figsize=(20, 5))\n",
    "for i, img in enumerate(tr.img_list):\n",
    "    ax = tr.axs[i]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.set_title(\"{:.0f}% {}, {:.0f}% {}\\n {:.0f}% {}, {:.0f}% {}\"\n",
    "                 .format(100 * tr.proba[i,0], tr.train_loader.dataset.classes[0],\n",
    "                         100 * tr.proba[i,1], tr.train_loader.dataset.classes[1],\n",
    "                         100 * tr.proba[i,2], tr.train_loader.dataset.classes[2],\n",
    "                         100 * tr.proba[i,3], tr.train_loader.dataset.classes[3],\n",
    "                  )\n",
    "    )\n",
    "    \n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covnert to TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.model_script = torch.jit.trace(orig.model, tr.test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model to use In RedisAI\n",
    "\n",
    "First, convert to CPU! Very important, since we'll use CPU in RedisAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = orig.model_script.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ml2rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml2rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml2rt.save_torch(orig.model_script, 'plant.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
